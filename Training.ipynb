{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a super-resolution model\n",
    "\n",
    "The dataset comes with several pretrained super-resolution models we used as a benchmark:\n",
    "\n",
    "- HighResNet\n",
    "- SRCNN Multi-Frame\n",
    "- SRCNN Single-Image\n",
    "\n",
    "We trained the models on a [p3.2xlarge](https://aws.amazon.com/ec2/instance-types/p3/) instance,\n",
    "and the training usually takes about 45 min - 1.5 hr on a single GPU instance, using 8 low-resolution revisits and the entire dataset.\n",
    "\n",
    "The splits we used are available in the `stratified_train_val_test_split.csv` file.  \n",
    "These splits are stratified to ensure equal representation of all LCCS/IPCC/SMOD classes within each split.  \n",
    "To run on a smaller subset, you can manually specify the number of AOIs to be used in each split using the `--train_split`, `--val_split`, `--test_split` arguments.\n",
    "\n",
    "To train the network, or reproduce this benchmark, you can run the following commands:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.train import *\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_train_command = [\n",
    "    # Batch size, gpus, limits\n",
    "    \"python\",\n",
    "    \"--batch_size\", \"48\",\n",
    "    \"--gpus\", \"-1\",\n",
    "    \"--max_steps\", \"50000\",\n",
    "    \"--precision\", \"16\",\n",
    "\n",
    "    # Model/Hyperparameters\n",
    "    \"--model\", \"highresnet\",\n",
    "    \"--w_mse\", \"0.3\",\n",
    "    \"--w_mae\", \"0.4\",\n",
    "    \"--w_ssim\", \"0.3\",\n",
    "    \"--hidden_channels\", \"128\",\n",
    "    \"--shift_px\", \"2\",\n",
    "    \"--shift_mode\", \"lanczos\",\n",
    "    \"--shift_step\", \"0.5\",\n",
    "    \"--residual_layers\", \"1\",\n",
    "    \"--learning_rate\", \"1e-4\",\n",
    "    \n",
    "    # Data\n",
    "    \"--dataset\", \"JIF\",\n",
    "    \"--root\", \"dataset\",\n",
    "    \"--revisits\", \"8\",\n",
    "    \"--input_size\", \"160\", \"160\",\n",
    "    \"--output_size\", \"500\", \"500\",\n",
    "    \"--chip_size\", \"50\", \"50\",\n",
    "    \"--radiometry_depth\", \"12\",\n",
    "\n",
    "    # Training, validation, test splits\n",
    "    \"--list_of_aois\", \"pretrained_model/stratified_train_val_test_split.csv\"\n",
    "]\n",
    "\n",
    "def run_training_command(training_command, running_on_windows=False):\n",
    "    sys.argv = training_command\n",
    "    if running_on_windows:\n",
    "        sys.argv += [\"--num_workers\", \"0\"]\n",
    "    cli_main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: Keep in mind the training was done on an instance with 1xV100 and 64 GB of RAM.  \n",
    "The batch size might be too large for your local computer.  \n",
    "\n",
    "If CUDA runs out of memory, consider decreasing it above in the `default_training_command`.  \n",
    "You can also decrease the number of revisits to any number from 1 to 8.\n",
    "\n",
    "If CUDA runs out of shared memory, you can increase it on Linux by running:  \n",
    "`sudo mount -o remount,size={YOUR_RAM_SIZE, e.g. 64G} /dev/shm`\n",
    "\n",
    "If running on Windows, set the `running_on_windows` flag in the `run_train_command` function to True."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a single model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1337\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mori\u001b[0m (\u001b[33mwhyhowltd\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.20"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\release\\esa_superres\\wandb\\run-20220702_155533-1qkg398d</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/whyhowltd/esasuperres/runs/1qkg398d\" target=\"_blank\">atomic-dust-2134</a></strong> to <a href=\"https://wandb.ai/whyhowltd/esasuperres\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using all LR bands.\n",
      "Shuffling the dataset splits using 42\n",
      "Train set size: 28287\n",
      "Val set size: 3528\n",
      "Test set size: 3528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ivano\\mambaforge\\envs\\worldstrat\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'backbone' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['backbone'])`.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\ivano\\mambaforge\\envs\\worldstrat\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:347: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "Using 16bit native Automatic Mixed Precision (AMP)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name             | Type                    | Params\n",
      "-------------------------------------------------------------\n",
      "0 | backbone         | HighResNet              | 3.3 M \n",
      "1 | baseline         | BicubicUpscaledBaseline | 0     \n",
      "2 | shifter          | Shift                   | 9.8 K \n",
      "3 | baseline_shifter | Shift                   | 9.8 K \n",
      "-------------------------------------------------------------\n",
      "3.3 M     Trainable params\n",
      "19.6 K    Non-trainable params\n",
      "3.3 M     Total params\n",
      "6.560     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d7988ab253a4a5cba2c742db9df0ef9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ivano\\mambaforge\\envs\\worldstrat\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\ivano\\mambaforge\\envs\\worldstrat\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58b7f43495bb4afab36f8a1f0424d5c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_training_command(default_train_command, running_on_windows=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reproducing the benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_random_seeds = [431608443, 122938034, 315114726]\n",
    "benchmark_data_seed = 386564310\n",
    "\n",
    "# HighResNet triple replicates\n",
    "highresnet_replicates = [\n",
    "    default_train_command \n",
    "    + [\"--data_split_seed\", str(benchmark_data_seed)]\n",
    "    + [\"--seed\", str(seed)] \n",
    "    for seed in benchmark_random_seeds\n",
    "]\n",
    "\n",
    "# SRCNN MultiFrame triple replicates\n",
    "# Change model to SRCNN\n",
    "\n",
    "default_train_command[10] = 'srcnn'\n",
    "srcnn_multiframe_replicates = [\n",
    "    default_train_command \n",
    "    + [\"--data_split_seed\", str(benchmark_data_seed)]\n",
    "    + [\"--seed\", str(seed)] \n",
    "    for seed in benchmark_random_seeds\n",
    "]\n",
    "\n",
    "# SRCNN Single Image triple replicates\n",
    "# Change number of revisits to 1\n",
    "default_train_command[34] = '1'\n",
    "srcnn_single_image_replicates = [\n",
    "    default_train_command \n",
    "    + [\"--data_split_seed\", str(benchmark_data_seed)]\n",
    "    + [\"--seed\", str(seed)] \n",
    "    for seed in benchmark_random_seeds\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for replicates in [highresnet_replicates, srcnn_multiframe_replicates, srcnn_single_image_replicates]:\n",
    "    for replicate_training_command in replicates:\n",
    "        run_training_command(replicate_training_command, running_on_windows=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('worldstrat')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4151d50128295e6419a637553904cae4fc18ed6dc9ed0de784cf617a79695b0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
